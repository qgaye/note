# 数据库(MySQL)

## ACID

- 原子性(Atomicity)：一个事务里的所有操作要么全都执行成功要么失败后全部回滚
- 一致性(Consistency)：数据库的完整性约束不被破坏，事务执行前后都是数据合法的状态
- 隔离性(Isolation)：并发执行的事务之间相互影响的程度，最严格的串行化表示事务间不会互相干扰
- 持久性(Durability)：事务一旦提交，对数据库的修改操作是永久的，不会被其他操作或故障所影响

MySQL实现ACID机制：

- 原子性：undo log
- 一致性：原子性，隔离性，持久性三者共同来保证一致性
- 隔离性：MVCC和锁机制
- 持久性：redo log

## MySQL为什么使用B+树

- IO操作往往是最耗时的，从树的根节点开始每下一层往往意味着一次IO查询，而B+树树高很低，因此也就意味着能够通过较少次数IO查询到记录
- B+树相较于B树非叶子节点只存储索引信息(B树上非叶子节点也会存储具体数据信息)，因此一次IO读取到的页中包含的索引信息量就更大，能更快查询到具体记录
- B+树所有数据都是存储在叶子节点上的，且所有叶子节点首尾相连，类似于链表，因此可以在B+树上做范围查询

### 为什么不能用业务id作为主键

- 业务id往往不能保证递增，而不是递增的主键的插入是随机IO，其次还可能造成页分裂
- 业务id字段长度未知，普通索引都存储了主键，如果业务id很长又作为了主键，会导致所有普通索引占用空间较大
- 业务id是很可能随着业务而更改的，比如字段类型发送变化等等，如果作为主键更改时会对数据库造成比较大影响
- 对切换到分布式架构不友好，因为业务id可能无法在分布式环境下保证唯一性

## 隔离级别选择

MySQL默认隔离级别是可重复读(RR)，而Oracle默认隔离级别是提交读(RC)

为什么MySQL默认隔离级别是可重复读：

因为早期MySQL中binlog只有statement一种形式，即直接保存提交的sql，因此sql语句执行的顺序就很关键，如果乱序就会导致数据异常(比如auto_increment执行顺序不同会导致值不同)，因此就需要在可重复读的隔离级别下通过间隙锁来保证sql执行顺序，但是现在binlog支持row格式，也就是不再一定需要保证sql执行顺序了

注：为什么MySQL页默认16kb，因为当以bigint(8B)+指针(6B)作为一条记录时，一个页上可以存储1000+条数据

因此MySQL数据库也推荐使用提交读的隔离级别：
- 提交读相较于可重复读而言，没有了next-key lock，因此发生死锁的可能性低了很多
- 在提交读的隔离级别下当条件未命中索引则不会像可重复读一样直接锁住全表，而是会释放掉不满足条件的记录

理论上在真实业务中，插入的数据被读到往往没有什么大问题

## MySQL加锁

- 快照读：select操作，通过MVCC实现，无需加锁，但如果select + for update则会使用当前读，是会加行锁的(可重复读下加间隙锁 + 行锁)
- 当前读：insert/update/delete，通过加行锁实现，并且一个事务中所有的行锁会在事务提交/回滚后才会全部释放

在可重复读的隔离级别下默认支持间隙锁，只需要显示为select语句加上for update/lock in share mode，可以自行关闭，那就只会加行锁，在可重复读之下的隔离级别是不支持间隙锁的

幻读是通过next-key lock实现的，也就是行锁 + 间隙锁

## buffer pool

buffer pool（缓冲池）即类似一个缓存机制，以避免每次查询都需要进行磁盘IO

预读：MySQL中是以页作为数据存储的基本单位，页的大小一般为16K。MySQL在读取时将整个页全部加载进buffer pool，因为该条数据的周围数据大概率会被继续读取（局部性原则），这样提前加载可以有效减少磁盘IO

但是buffer pool是会满的，MySQL使用新老生代的LRU算法来有效避免普通LRU算法带来的缓冲池污染的问题

新老生代LRU算法：

整个buffer pool前70%被分为新生代，后30%被分为老生代
- 当读取的数据不在buffer pool中：
    - 查询出该数据，直接插入到老生代的head，如果buffer pool满了则淘汰掉老生代中的tail（也就是整个buffer pool的tail）再插入到老生代的head
- 当读取的数据在buffer pool中：
    - 如果数据位于新生代中，则将它移动到到新生代的head（也就是整个buffer pool的head）
    - 如果数据位于老生代中，并且该数据在老生代中停留时间大于1s（由参数控制）时，移动到新生代的head，否则保持原先位置不变

当批量扫描大量数据时，因为移动到head是要求数据在老生代中达到一定停留时间的，所以这些非热点数据只会在老生代中不断被替换，而不会影响到处在新生代中真正的热点数据

## redo log

当对数据进行增删改操作时，首先需要将数据从磁盘上读取到buffer pool(相当于缓存)中，然后在buffer pool中进行修改，但此时buffer pool上被修改过的数据页就和磁盘上的数据页不一致，这些数据页被称为脏页。当数据库崩溃后，这些脏页的数据因为没有被持久化到磁盘就会丢失，因此InnoDB设计了redo log，每次数据更新后将数据页发生的修改信息持久化到磁盘，从而保证在奔溃后可以通过redo log直接恢复数据页上的信息

- redo log优势在于其每次操作是一个顺序IO，相较于每次直接将脏页刷入磁盘的随机IO，性能更佳
- redo log作用主要用于数据重做，即脏页数据能在奔溃后恢复到磁盘，而binlog主要用于不同数据库间数据同步
- redo log记录的是数据页上的变化信息(比如这个页号的页的offset处更新为新值)，而不是像binlog中存储的是具体sql
- redo log在整个事务执行过程中都会不断的持久化到磁盘(定时刷盘或组提交)，而binlog则是在整个事务提交后才会落盘
- redo log在磁盘上是指定存储到`ib_logfile1/2/3...`文件中，文件数量和大小是配置的，因此如果一旦redo log文件满了，就需要将记录应用到真正的数据磁盘上(即脏页落盘)，redo log file中有一个checkpoint，checkpoint之后表示还未刷盘的redo log，即数据库奔溃后需要恢复应用的redo log，而binlog是无限追加写到文件中的

因为未提交和回滚的事务也都记录了redo log，因此当恢复时对这些事务需要特殊处理：在InnoDB进行恢复时，会重做所有事务的redo log，接着对未提交的事务执行undo log
- 因为未提交的事务在恢复时需要执行undo log，因此undo log会被看作数据一起被记录在redo log中，当发现恢复的事务是未提交的，那就会执行undo log来回滚这些操作
- 对于回滚的事务，会直接在redo log中与本身事务相反的操作，比如插入数据在回滚后会在redo log中接着插入条删除数据的记录

## undo log

MVCC中通过undo log实现事务的回滚和回溯获取到当前事务所在版本的数据，MVCC中会将最新版本的数据直接更新在该行记录上(即使该版本数据未提交)，旧版本的数据全部存储在undo log上，当事务需要读取旧版本数据时通过undo log找到该版本数据返回(对于回滚事务不是依赖undo log的，而是在redo log中会记录与本事务相反的操作来完成回滚操作)

InnoDB为每一行都添加三个字段：
- DB_TRX_ID(6B)：标示最近一次对该行做修改(update/insert)的事务的事务id
- DB_ROLL_PTR(7B)：指针，指向重建该行被更新前的内容的信息的undo log具体信息
- DB_ROW_ID(6B)：随着新行插入而单调递增的ID，当未设置主键时会使用该行值作为主键(和undo log没关系)

在事务开始时会将此时活跃的事务(创建但未提交)的事务ID全部记录到一个数组中，这个数组中最小值称作低水位，最大值称作高水位

当其他事务访问该行时，首先与该行上的DB_TRX_ID比较，相同就说明最新版的数据就是该事务修改的，可以直接读取，反之从undo log中找到前一个版本的事务号与事务开始前记录的数组进行比较
- `低水位 >= transaction id`表示该数据版本在事务前已被提交，该数据可用，相等的情况表示该数据版本是该事务提交的，因此可用
- `高水位 < transaction id`表示该数据版本是在事务启动后才被更新，该数据不可用
- `低水位 < transaction id <= 高水位`此时判断该事务是否在该事务的数组中，在表示不可用，不在表示可用(出现这种情况的原因是可能在该事务开启前的事务有些很快提交了，而有些一直未提交)

这就是可见性比较算法

## 半同步下主从切换问题

半同步模式下，master会将binlog发送给slave，并在接收到一台slave的ack确认后才返回确认给客户端，整个过程实质上只是保证了binlog能够被一台slave接收到(只要收到一台slave的ack就返回，因此无法保证所有slave都已经收到binlog了)，但是slave接收到binlog后将其真正应用到自身数据库上是需要时间的，那么这时如果master崩了，切换到slave时也不能够立刻投入使用的，因为此时存在部分master传来的binlog还没有应用的问题，如果直接切换，那么存在数据不一致的问题

总结：即使在半同步下主从切换也是需要保证从库将所有binlog都应用完成后才能说明主从数据一致了，如果将业务流量直接切换到从库，那么auto_increment主键是很容易出现冲突问题的

## explain

- select_type：查询类型
  - Simple：不带子查询或union查询
  - Primary：该查询是最外层的查询
  - Union：表示是union第二个或后面的查询
  - Subquery：子查询中第一个select
- table：查询涉及的表或衍生表
- partitions：匹配的分区
- type：
  - system：表只有一行，const的特殊情况
  - const：最多只有一行匹配，比如使用主键或唯一索引
  - eq_ref：只匹配到一行
  - ref：符合索引的最左前缀
  - range：范围查找
  - index：全表扫描，但扫描的是索引树
  - all：全表扫描
  - type性能：ALL < index < range ~ index_merge < ref < eq_ref < const < system
- possible_keys：在该查询中可以用到的索引，但正真用到的索引在key中显示
- key：正真被用到的索引
- key_len：使用到的索引的字节数，可以用来判断复合索引中最左前缀匹配到那些字段
- ref：表示表查找用到的常量const，字段，函数
- rows：估算的需要扫描到的行数
- filtered：表示该查询条件过滤的数据的百分比
- extra：
  - using filesort：order by/group by没能走索引，使用内存/文件排序
  - using temporay：用了临时表保存中间结果
  - using index：用到了覆盖索引
  - using index condition：用到了索引下推
  - using where：表示存储引擎返回给MySQL服务层后再做where过滤
  - using join buffer：使用了join的连接缓存，BNL/BKA

## 分页

limit 0, 5 => (1 ~ 5)
limit 5 => (1 ~ 5)
limit 10, 15 => (11 ~ 25)
limit 15 offset 10 => (11 ~ 25)

当limit查询到很后面的行时，比如`limit 10000, 20`实质上是会扫描满足条件的10020行数据，那么此时查询的速度就很慢了

分页优化：

1. 如果主键是递增的(auto_increment)，那么可以先查出主键(只查询主键是无需回表的，既可以利用覆盖索引)，然后用in，join，between，大于小于符号 方式查出具体数据
2. 对于offset很后面的内容，使用倒序(DESC)来查询

### Hash索引

Hash索引只需要一次hash计算后定位即可找到数据，而无需像B+树多次IO访问从根节点访问到最后的叶子节点

Hash索引查询效率高，但存在如下问题：
- Hash索引无法支持范围查询
- Hash索引无法用于数据的排序操作
- Hash索引无法像B+树中的联合索引一样利用部分(最左匹配)索引查询
- Hash索引中当数据量大时，存在大量Hash冲突，此时一个Hash值就对应着多条记录，在查询时就必须将这些记录都遍历一遍后才能返回，性能反而更差

### drop/delete/truncate

drop和truncate是DDL语句，执行后会自动提交，而delete是需要事务提交的。且会触发相应的触发器

truncate和delete都只删除数据而不删除表结构，但删除整个表数据时truncate性能更好且会重置auto_increment值

drop不但会删除表中所有数据，还会删除该表的约束，触发器，索引等，且相关函数将不可用

### 分布式主键ID

- 自增ID，分库分表插入数据前都先向同一张表插入一条无意义数据以获得一个保证唯一的主键ID，但性能差
- 自增ID配合offset和increment(step)，提前为每台机子设定好step，从而保证之间主键不会冲突，但是如果需要添加新的机子就需要重新配置所有的step值
- UUID，虽然本身保证了主键的唯一性，但是UUID是无序的，性能太差
- snowflake算法，1bit为0表示正数，41bit表示时间戳，10bit表示机器ID，12bit记录同一毫秒内不同的主键，当新的机器加入集群时，可以先来主动请求获取一个它的机器ID，从而保证唯一性和扩展性，此外因为前41bit是时间戳，因此该主键ID还能保证递增性
- 美团Leaf：每个服务预先请求一段主键ID，这段ID在各自服务内递增使用，当使用完后再去请求下一段主键ID，因此当ID用完后请求新ID端时会发生卡顿

### 排查慢sql

MySQL中需要在`my.cnf`的`[mysqld]`中开启慢查询日志，该日志会记录所有执行时间超过`long_query_time`的sql信息

```txt
slow_query_log=1   # 打开慢查询日志
slow_query_log_file=/var/log/mysql/log-slow-queries.log    # 慢查询日志存储文件
long_query_time=2     # 设置超时时间，即超过该配置秒数就进行记录
```

慢sql原因：
- redo log满了，checkpoint需要向后推，因此需要将redo log中记录应用到数据磁盘上
- buffer pool满了，就需要腾出一部分内存空间给新的查询数据
- 长事务，导致undo log堆积
- 等待行锁，比如其他事务对这行进行了修改因此加了行锁，但是行锁需要在事务结束后才释放，如果这个事务执行很长时间，就会导致其他事务无法获取到该行锁
- 错选索引：
  - order by：sql中走a索引遍历行数较少，但因为b字段加了order by，因此选择走了b索引，此时可以给a也加上order by
  - 索引的行数不是精确计算的，而是随机选择N个数据页，计算平均值后乘以总页数得到基数，普通索引有个回表过程，因此优化器可能计算结果后选择直接走主键遍历全表而不是走普通索引，此时可以重建索引
- 能走索引但没走索引：索引参与加减乘除运算，索引参与函数运算，隐式类型转换，隐式字符编码转换

索引建立原则：
- 索引覆盖：将一些查询返回的列也一起放在索引中(即使该列无需索引)，从而在返回时可通过该索引直接返回，而无需再回表
- 最左前缀匹配原则：可以将多个单列索引组合成一个联合索引，使用频率较高的列放在左侧，从而匹配最左原则。减少索引数量也可以有效提升修改速度
- 使用高区分度的列作为索引：区分度低的列作为索引时，会导致索引的B+树树高很低，大量字段值被挂在同一个索引值下。可以通过`count(distinct col) / count(*)`得到该列中字段不重复比例，根据实际情况判断是否能作为索引

慢sql解决：
- 优化sql，比如join的被驱动列建立索引，order by上建立索引
- 加大buffer pool大小
- 排查高并发下行锁造成超时问题
- 如果数据量过大，需要进一步分库分表设计
